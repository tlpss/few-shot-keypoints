{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55487bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 640\n",
      "processed image shape: torch.Size([1, 3, 224, 224])\n",
      "13\n",
      "last hidden states shape: torch.Size([1, 257, 384])\n",
      "cls token shape: torch.Size([1, 384])\n",
      "patch features shape: torch.Size([1, 16, 16, 384])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.models.dinov2.modeling_dinov2 import Dinov2Model\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "print(image.height, image.width)  # [480, 640]\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "model = Dinov2Model.from_pretrained('facebook/dinov2-small')\n",
    "\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "print(f\"processed image shape: {inputs.pixel_values.shape}\")  # [1, 3, 224, 224]\n",
    "\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "print(len(outputs.hidden_states))\n",
    "last_hidden_states = outputs[0]\n",
    "print(f\"last hidden states shape: {last_hidden_states.shape}\")  # [1, 1 + 256, 768]\n",
    "\n",
    "num_patches_height = inputs.pixel_values.shape[2] // model.config.patch_size\n",
    "num_patches_width = inputs.pixel_values.shape[3] // model.config.patch_size\n",
    "cls_token = last_hidden_states[:, 0, :]\n",
    "patch_features = last_hidden_states[:, 1:, :].unflatten(1, (num_patches_height, num_patches_width))\n",
    "\n",
    "print(f\"cls token shape: {cls_token.shape}\")\n",
    "print(f\"patch features shape: {patch_features.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.dinov2.modeling_dinov2.Dinov2Model'>\n",
      "<class 'transformers.models.dinov2.configuration_dinov2.Dinov2Config'>\n",
      "<class 'transformers.models.bit.image_processing_bit.BitImageProcessor'>\n",
      "BitImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"BitImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 256\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(model.config))\n",
    "print(type(processor))\n",
    "print(processor)\n",
    "\n",
    "# preprocessing in original codebase: https://github.com/facebookresearch/dinov2/blob/main/dinov2/data/transforms.py#L63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d8278f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed image shape: torch.Size([1, 3, 1024, 1024])\n",
      "last hidden states shape: torch.Size([1, 5330, 384])\n",
      "cls token shape: torch.Size([1, 384])\n",
      "patch features shape: torch.Size([1, 73, 73, 384])\n",
      "patches x patch size: 1022 x 1022\n"
     ]
    }
   ],
   "source": [
    "# change the input image size \n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small',size={\"width\": 1024, \"height\": 1024},do_center_crop=False)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "print(f\"processed image shape: {inputs.pixel_values.shape}\")  # [1, 3, 224, 224]\n",
    "\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "last_hidden_states = outputs[0]\n",
    "print(f\"last hidden states shape: {last_hidden_states.shape}\")  # [1, 1 + 256, 768]\n",
    "\n",
    "num_patches_height = inputs.pixel_values.shape[2] // model.config.patch_size\n",
    "num_patches_width = inputs.pixel_values.shape[3] // model.config.patch_size\n",
    "cls_token = last_hidden_states[:, 0, :]\n",
    "patch_features = last_hidden_states[:, 1:, :].unflatten(1, (num_patches_height, num_patches_width))\n",
    "\n",
    "print(f\"cls token shape: {cls_token.shape}\")\n",
    "print(f\"patch features shape: {patch_features.shape}\")\n",
    "\n",
    "print(f\"patches x patch size: {num_patches_height*model.config.patch_size} x {num_patches_width*model.config.patch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c92f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6109ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 518, 518])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'patch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(inputs.pixel_values.shape)  \u001b[38;5;66;03m# [1, 3, 224, 224]\u001b[39;00m\n\u001b[32m      9\u001b[39m batch_size, rgb, img_height, img_width = inputs.pixel_values.shape\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m num_patches_height, num_patches_width = img_height // \u001b[43mpatch_size\u001b[49m, img_width // patch_size\n\u001b[32m     11\u001b[39m num_patches_flat = num_patches_height * num_patches_width\n\u001b[32m     13\u001b[39m outputs = model(**inputs, output_hidden_states=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'patch_size' is not defined"
     ]
    }
   ],
   "source": [
    "# now do the same for the Dinov2-large model\n",
    "\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-large')\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large',size={\"height\":518,\"width\":518},do_center_crop=False)\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "print(inputs.pixel_values.shape)  # [1, 3, 224, 224]\n",
    "batch_size, rgb, img_height, img_width = inputs.pixel_values.shape\n",
    "num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n",
    "num_patches_flat = num_patches_height * num_patches_width\n",
    "\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "print(len(outputs.hidden_states))\n",
    "last_hidden_states = outputs[0]\n",
    "print(last_hidden_states.shape)  # [1, 1 + 256, 768]\n",
    "assert last_hidden_states.shape == (batch_size, 1 + num_patches_flat, model.config.hidden_size)\n",
    "print(num_patches_height,num_patches_width)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "488a9fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "runwayml/stable-diffusion-v1-5 does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:467\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     config_dict, _ = \u001b[43mImageProcessingMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:366\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresolved_image_processor_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[32m    367\u001b[39m         text = reader.read()\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# build sd1.5 preprocessor and print\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m processor = \u001b[43mAutoImageProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrunwayml/stable-diffusion-v1-5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(processor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:471\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    467\u001b[39m     config_dict, _ = ImageProcessingMixin.get_image_processor_dict(\n\u001b[32m    468\u001b[39m         pretrained_model_name_or_path, image_processor_filename=CONFIG_NAME, **kwargs\n\u001b[32m    469\u001b[39m     )\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m initial_exception\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# because only timm models have image processing in `config.json`.\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_timm_config_dict(config_dict):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:458\u001b[39m, in \u001b[36mAutoImageProcessor.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;66;03m# Load the image processor config\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    457\u001b[39m     \u001b[38;5;66;03m# Main path for all transformers models and local TimmWrapper checkpoints\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     config_dict, _ = \u001b[43mImageProcessingMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_processor_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m initial_exception:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;66;03m# instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# except the model name, the only way to check if a remote checkpoint is a timm model is to try to\u001b[39;00m\n\u001b[32m    465\u001b[39m     \u001b[38;5;66;03m# load `config.json` and if it fails with some error, we raise the initial exception.\u001b[39;00m\n\u001b[32m    466\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/image_processing_base.py:338\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m image_processor_file = image_processor_filename\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     resolved_image_processor_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_processor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/few-shot-keypoints/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:573\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     revision_ = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m revision\n\u001b[32m    570\u001b[39m     msg = (\n\u001b[32m    571\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33ma file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_entries[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_entries) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfiles named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(*missing_entries,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    572\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    574\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Checkout \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    575\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m for available files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     )\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# Remove potential missing entries (we can silently remove them at this point based on the flags)\u001b[39;00m\n\u001b[32m    579\u001b[39m resolved_files = [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[31mOSError\u001b[39m: runwayml/stable-diffusion-v1-5 does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main' for available files."
     ]
    }
   ],
   "source": [
    "# build sd1.5 preprocessor and print\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "print(processor)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
