{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289079a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e868ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "DATA_DIR = \"/home/tlips/Code/few-shot-keypoints/data/RPL-clean-pen/3mark_1arm_fullresolution\"\n",
    "\n",
    "\n",
    "demo_dirs = os.listdir(DATA_DIR)\n",
    "# filter out directories that are not demos\n",
    "demo_dirs = [d for d in demo_dirs if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "# sort by name\n",
    "demo_dirs.sort()\n",
    "print(f\"Found {len(demo_dirs)} demonstrations\")\n",
    "\n",
    "\n",
    "# Ã°emo dir \n",
    "#  back_camera\n",
    "# data.mp4\n",
    "\n",
    "video_paths = [os.path.join(DATA_DIR, demo_dir, \"back_camera\", \"data.mp4\") for demo_dir in demo_dirs]\n",
    "print(video_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get initial frames for each video\n",
    "\n",
    "initial_frames = []\n",
    "for video_path in video_paths:\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    ret, frame = video.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    initial_frames.append(frame)\n",
    "\n",
    "# save the initial frames\n",
    "initial_frames = np.array(initial_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce5064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instead, now get all frames of a single video\n",
    "url = \"/home/tlips/Code/few-shot-keypoints/data/RPL-clean-pen/3mark_1arm_fullresolution/demo_1/back_camera/data.mp4\"\n",
    "\n",
    "import imageio.v3 as iio\n",
    "frames = iio.imread(url, plugin=\"FFMPEG\")  # plugin=\"pyav\"\n",
    "# frames = frames[-1100:-800]\n",
    "print(len(frames))\n",
    "initial_frames = frames[::40]\n",
    "print(len(initial_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\n",
    "model_id =  \"IDEA-Research/grounding-dino-tiny\"\n",
    "# model_id = \"google/owlv2-base-patch16\"\n",
    "device = \"cuda\"\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594666f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# will only compile on first call\n",
    "# img = np.random.randint(0, 255, (100, 100, 3))\n",
    "# text_labels = [\"dummy\"]\n",
    "# inputs = processor(text=text_labels, images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# model.compile()\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels = [\"pen\", \"box\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "color_list = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255), (255, 255, 255), (0, 0, 0)]\n",
    "\n",
    "\n",
    "img = initial_frames[0]\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "def get_bounding_boxes(img, text_labels):\n",
    "    # https://github.com/huggingface/transformers/blob/main/src/transformers/models/grounding_dino/processing_grounding_dino.py\n",
    "    \n",
    "    text_labels = [text_labels]\n",
    "    inputs = processor(text=text_labels, images=img, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    results = processor.post_process_grounded_object_detection(outputs, inputs.input_ids,threshold=0.3, target_sizes=[img.shape[:2]])\n",
    "    return results[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = get_bounding_boxes(img, text_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1068775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes_on_image(img, results, text_labels):\n",
    "    annotated_img = img.copy()\n",
    "    # check if any annotations\n",
    "    if len(results['boxes']) is None or len(results['boxes']) == 0:\n",
    "        return annotated_img\n",
    "    for box, label, score in zip(results['boxes'], results['labels'], results['scores']):\n",
    "    \n",
    "        x1, y1, x2, y2 = box\n",
    "        x1 = int(x1)\n",
    "        y1 = int(y1)\n",
    "        x2 = int(x2)\n",
    "        y2 = int(y2)\n",
    "        label_idx = text_labels.index(label)\n",
    "        annotated_img = cv2.rectangle(annotated_img, (x1, y1), (x2, y2), color_list[label_idx], 2)\n",
    "        annotated_img = cv2.putText(annotated_img, f\"{label}: {score:.2f}\", (x1, y2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    return annotated_img\n",
    "\n",
    "\n",
    "annotated_img = visualize_boxes_on_image(img, results, text_labels)\n",
    "plt.imshow(annotated_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on all frames and store the results in a list \n",
    "\n",
    "initial_frame_results  =  []\n",
    "times = []\n",
    "import time \n",
    "for frame in initial_frames:\n",
    "    start_time = time.time()\n",
    "    # resize frame to 256x256\n",
    "    # frame = cv2.resize(frame, (256, 256))\n",
    "    results = get_bounding_boxes(frame, text_labels)\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "    initial_frame_results.append(results)\n",
    "\n",
    "\n",
    "print(f\"Average time per frame: {sum(times) / len(times)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize in a single figure with 5 by len(initial_frames) // 5 + 1 grid\n",
    "\n",
    "fig, axs = plt.subplots(len(initial_frames) // 5 + 1, 5, figsize=(25, 5 * len(initial_frames) // 5))\n",
    "for i, frame in enumerate(initial_frames):\n",
    "    # frame = cv2.resize(frame, (256, 256))\n",
    "    img = visualize_boxes_on_image(frame, initial_frame_results[i], text_labels)\n",
    "    axs[i // 5, i % 5].imshow(img)\n",
    "    axs[i // 5, i % 5].axis(\"off\")\n",
    "    axs[i // 5, i % 5].set_title(f\" {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad44ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
